@InProceedings{arya17:improved,
  title =	 {An improved method for DNA sequence compression},
  author =	 {Arya, Govind Prasad and Bharti, RK and Prasad,
                  Devendra and Garg, Vishal},
  booktitle =	 {2nd International Conference on Telecommunication
                  and Networks ({TEL-NET})},
  pages =	 {1--4},
  month =	 8,
  year =	 2017,
  address =	 {Noida, India},
  organization = {IEEE},
  abstract =	 {DNA (deoxyribonucleic acid), is the hereditary
                  material in humans and almost all other
                  organisms. Nearly every cell in a person's body has
                  the same DNA. The information in DNA is stored as a
                  code made up of four chemical bases: adenine (A),
                  guanine (G), cytosine (C), and thymine (T). With
                  continuous technology development and growth of
                  sequencing data, large amount of biological data is
                  generated. This large amount of generated data
                  causes difficulty to store, analyses and process DNA
                  sequences. Therefore, a wide need of reducing the
                  size, for this reason, DNA Compression is employed
                  to reduce the size of DNA sequence. Therefore, there
                  is a huge need of compressing the DNA sequence. In
                  this paper, we have proposed an efficient and fast
                  DNA sequence compression algorithm based on
                  differential direct coding and variable look up
                  table (LUT).},
  doi =		 {10.1109/TEL-NET.2017.8343579},
}

@Article{bakr13:dna,
  title =	 {DNA lossless compression algorithms},
  author =	 {Bakr, Nour S and Sharawi, Amr A and others},
  journal =	 {American Journal of Bioinformatics Research},
  volume =	 3,
  number =	 3,
  pages =	 {72--81},
  year =	 2013,
  doi =		 {10.5923/j.bioinformatics.20130303.04},
  publisher =	 {Scientific \& Academic Publishing},
  address =	 {Rosemeand, CA, USA}
}

@InProceedings{balleda14:genseek,
  title =	 {GenSeeK: A Novel Parallel Multiple Pattern
                  Recognition Algorithm for DNA Sequences},
  author =	 {Balleda, Kaliuday and Satyanvesh, D and Baruah, PK},
  booktitle =	 {Proceedings of the International Conference on
                  Advanced Computing, Networking, and Informatics},
  pages =	 {1001--1006},
  month =	 6,
  year =	 2014,
  organization = {Springer},
  abstract =	 {DNA sequences are huge in size, and the genome
                  databases are growing exponentially every year. One
                  of the key elements in computational biology is
                  genomic data. There are many real-time applications,
                  such as DNA profiling and real-time crime
                  investigation, which requires the biological
                  subjects DNA sequences at real time. To retrieve
                  this, data in real time require lot of computational
                  power and resources. Throughput is one of the main
                  bottleneck for applications such as DNA sequence
                  searching or pattern matching. This paper presents a
                  new DNA sequence multiple pattern recognition
                  algorithm which computes on compressed space. This
                  algorithm is efficient in terms of computational
                  complexity and the amount of resources required
                  during the computation in real time, the main reason
                  for this behavior is that it does the computations
                  on compressed sequences. This algorithm is
                  implemented using index-based technique, and the
                  sequential code is optimized. The proposed algorithm
                  is mainly focused on achieving good comparison per
                  character ratio as well as high throughput. The
                  parallel version of the algorithm is implemented
                  using multicore for achieving high throughput. The
                  techniques used in development of this algorithm can
                  be directly translated into huge DNA database
                  search.}
}

@Article{bharti11:biological,
  title =	 {A biological sequence compression based on cross
                  chromosomal similarities using variable length lut},
  author =	 {Bharti, Rajendra Kumar and Verma, Archana and Singh,
                  R},
  journal =	 {International Journal of Biometrics and
                  Bioinformatics},
  volume =	 4,
  number =	 6,
  pages =	 217,
  year =	 2011,
  publisher =	 {CS Journals},
  abstract =	 {While modern hardware can provide vast amounts of
                  inexpensive storage for biological databases, the
                  compression of Biological sequences is still of
                  paramount importance in order to facilitate fast
                  search and retrieval operations through a reduction
                  in disk traffic. This issue becomes even more
                  important in light of the recent increase of very
                  large data sets, such as meta genomes.  The present
                  Biological sequence compression algorithms work by
                  finding similar repeated regions within the
                  Biological sequence and then encode these repeated
                  regions together for compression. The previous
                  research on chromosome sequence similarity reveals
                  that the length of similar repeated regions within
                  one chromosome is about 4.5\% of the total sequence
                  length. The compression gain is often not high
                  because of these short lengths of repeated
                  regions. It is well recognized that similarities
                  exist among different regions of chromosome
                  sequences. This implies that similar repeated
                  sequences are found among different regions of
                  chromosome sequences. Here, we apply
                  cross-chromosomal similarity for a Biological
                  sequence compression. The length and location of
                  similar repeated regions among the different
                  Biological sequences are studied. It is found that
                  the average percentage of similar subsequences found
                  between two chromosome sequences is about 10\% in
                  which 8\% comes from crosschromosomal prediction and
                  2\% from self-chromosomal prediction. The percentage
                  of similar subsequences is about 18\% in which only
                  1.2\% comes from self-chromosomal prediction while
                  the rest is from cross-chromosomal prediction among
                  the different Biological sequences studied. This
                  suggests the significance of cross-chromosomal
                  similarities in addition to self-chromosomal
                  similarities in the Biological sequence
                  compression. An additional 23\% of storage space
                  could be reduced on average using self-chromosomal
                  and crosschromosomal predictions in compressing the
                  different Biological sequences.}
}

@Article{bose12:bind,
  title =	 {BIND--An algorithm for loss-less compression of
                  nucleotide sequence data},
  author =	 {Bose, Tungadri and Mohammed, Monzoorul Haque and
                  Dutta, Anirban and Mande, Sharmila S},
  journal =	 {Journal of biosciences},
  volume =	 37,
  pages =	 {785--789},
  year =	 2012,
  doi =		 {10.1007/s12038-012-9230-6},
  publisher =	 {Springer},
  abstract =	 {Recent advances in DNA sequencing technologies have
                  enabled the current generation of life science
                  researchers to probe deeper into the genomic
                  blueprint. The amount of data generated by these
                  technologies has been increasing exponentially since
                  the last decade. Storage, archival and dissemination
                  of such huge data sets require efficient solutions,
                  both from the hardware as well as software
                  perspective. The present paper describes BIND – an
                  algorithm specialized for compressing nucleotide
                  sequence data. By adopting a unique ‘block-length’
                  encoding for representing binary data (as a key
                  step), BIND achieves significant compression gains
                  as compared to the widely used general purpose
                  compression algorithms (gzip, bzip2 and
                  lzma). Moreover, in contrast to implementations of
                  existing specialized genomic compression approaches,
                  the implementation of BIND is enabled to handle
                  non-ATGC and lowercase characters. This makes BIND a
                  loss-less compression approach that is suitable for
                  practical use. More importantly, validation results
                  of BIND (with real-world data sets) indicate
                  reasonable speeds of compression and decompression
                  that can be achieved with minimal processor/memory
                  usage. BIND is available for download at
                  http://metagenomics.atc.tcs.com/compression/BIND. No
                  license is required for academic or non-profit use.}
}

@InProceedings{cao07:simple,
  title =	 {A simple statistical algorithm for biological
                  sequence compression},
  author =	 {Cao, Minh Duc and Dix, Trevor I and Allison, Lloyd
                  and Mears, Chris},
  booktitle =	 {2007 Data Compression Conference (DCC'07)},
  pages =	 {43--52},
  year =	 2007,
  annote =	 {The original XM paper},
  abstract =	 {This paper introduces a novel algorithm for
                  biological sequence compression thatmakes use of
                  both statistical properties and repetition within
                  sequences.  A panel ofexperts is maintained to
                  estimate the probability distribution of the next
                  symbol inthe sequence to be encoded.  Expert
                  probabilities are combined to obtain the final
                  dis-tribution.  The resulting information sequence
                  provides insight for further study ofthe biological
                  sequence.  Each symbol is then encoded by arithmetic
                  coding.  Experi-ments show that our algorithm
                  outperforms existing compressors on typical DNA
                  andprotein sequence datasets while maintaining a
                  practical running time.},
  url =
                  {https://users.monash.edu/~lloyd/tildeStrings/Compress/2007DCC/preprint.pdf},
  organization = {IEEE}
}

@Article{cevallos22:brief,
  title =	 {A brief review on dna storage, compression, and
                  digitalization},
  author =	 {Cevallos, Yesenia and Nakano, Tadashi and
                  Tello-Oquendo, Luis and Rushdi, Ahmad and Inca,
                  Deysi and Santill{\'a}n, Ivone and Shirazi, Amin
                  Zadeh and Samaniego, Nicolay},
  journal =	 {Nano Communication Networks},
  volume =	 31,
  pages =	 100391,
  year =	 2022,
  doi =		 {10.1016/j.nancom.2021.100391},
  publisher =	 {Elsevier}
}

@InProceedings{challa16:novel,
  title =	 {A novel compression technique for DNA sequence
                  compaction},
  author =	 {Challa, Ratnakumari and Devi, G Pranayani and Arava,
                  Karuna and SrinivasaRao, Kanusu},
  booktitle =	 {Proceedings of the International Conference on
                  Signal Processing, Communication, Power and Embedded
                  System (SCOPES)},
  pages =	 {1351--1354},
  month =	 10,
  year =	 2016,
  doi =		 {10.1109/SCOPES.2016.7955660},
  organization = {IEEE},
  abstract =	 {Modern Biotechnology produces large amount of
                  genomic data. The explosion of DNA data has given a
                  challenge for understanding genomic structure, the
                  disk storage and computation. It is essential for
                  the development of efficient compression techniques
                  to handle genomic data storage. Data compression is
                  used to store the data in less memory. The
                  properties of DNA sequence offer a chance to build
                  DNA specific compression algorithms. In this paper,
                  a novel compression technique is proposed for
                  genomic data. In the first stage, each base in DNA
                  sequence is converted into binary form using 2-bit
                  encoding system. On the resultant binary string, A
                  Modified run length encoding is applied. The output
                  is compressed again using Huffman encoding technique
                  in second stage. The encoded sequence is converted
                  into ASCII characters. This technique is quite
                  simple and effective.}
}

@InProceedings{chen00:compression,
  title =	 {A compression algorithm for DNA sequences and its
                  applications in genome comparison},
  author =	 {Chen, Xin and Kwong, Sam and Li, Ming},
  booktitle =	 {Proceedings of the fourth annual international
                  conference on Computational molecular biology},
  pages =	 107,
  annote =	 {The original Gen-Compress paper},
  doi =		 {10.1145/332306.332352},
  year =	 2000,
  abstract =	 {We present a lossless compression algorithm,
                  Gen-Compress, for DNA sequences, based on searching
                  for approximate repeats. Our algorithm achieves the
                  best compression ratios for benchmark DNA sequences,
                  comparing to other DNA compression programs [3,
                  7]. Significantly better compression results show
                  that the approximate repeats are one of the main
                  hidden regularities in DNA sequences.  We then
                  describe a theory of measuring the relatedness
                  between two DNA sequences. We propose to use d(x, y)
                  = 1 — K(x) - K(x|y)/K(xy to measure the distance of
                  any two sequences, where K stands for Kolmogorov
                  complexity [5]. Here, K(x) - K(x|y) is the mutual
                  information shared by x and y. But mutual
                  information is not a distance, there is no triangle
                  inequality. The distance d(x, y) is symmetric. It
                  also satisfies the triangle inequality, and
                  furthermore, it is universal [4].  It has not
                  escaped our notice that the distance measure we have
                  postulated can be immediately used to construct
                  evolutionary trees from DNA sequences, especially
                  those that cannot be aligned, such as complete
                  genomes. With more and more genomes sequenced,
                  constructing trees from genomes becomes possible [1,
                  2, 6, 8]. Kolmogorov complexity is not
                  computable. We use GenCompress to approximate it. We
                  present strong experimental support for this theory,
                  and demonstrate its applicability by correctly
                  constructing a 16S (18S) rRNA tree, and a whole
                  genome tree for several species of bacteria. Larger
                  scale experiments are underway at the University of
                  Waterloo, with very promising results}
}

@Article{chen02:dnacompress,
  title =	 {DNACompress: fast and effective DNA sequence
                  compression},
  author =	 {Chen, Xin and Li, Ming and Ma, Bin and Tromp, John},
  journal =	 {Bioinformatics},
  volume =	 18,
  number =	 12,
  pages =	 {1696--1698},
  year =	 2002,
  abstract =	 {While achieving the best compression ratios for DNA
                  sequences, our new DNACompress program significantly
                  improves the running time of all previous DNA
                  compression programs.}
}

@Article{cox12:bwt,
  author =	 {Cox, Anthony J. and Bauer, Markus J. and Jakobi,
                  Tobias and Rosone, Giovanna},
  title =	 "{Large-scale compression of genomic sequence
                  databases with the Burrows–Wheeler transform}",
  journal =	 {Bioinformatics},
  volume =	 28,
  number =	 11,
  pages =	 {1415-1419},
  year =	 2012,
  month =	 05,
  abstract =	 {Motivation: The Burrows–Wheeler transform (BWT) is
                  the foundation of many algorithms for compression
                  and indexing of text data, but the cost of computing
                  the BWT of very large string collections has
                  prevented these techniques from being widely applied
                  to the large sets of sequences often encountered as
                  the outcome of DNA sequencing experiments. In
                  previous work, we presented a novel algorithm that
                  allows the BWT of human genome scale data to be
                  computed on very moderate hardware, thus enabling us
                  to investigate the BWT as a tool for the compression
                  of such datasets.Results: We first used simulated
                  reads to explore the relationship between the level
                  of compression and the error rate, the length of the
                  reads and the level of sampling of the underlying
                  genome and compare choices of second-stage
                  compression algorithm.We demonstrate that
                  compression may be greatly improved by a particular
                  reordering of the sequences in the collection and
                  give a novel ‘implicit sorting’ strategy that
                  enables these benefits to be realized without the
                  overhead of sorting the reads. With these
                  techniques, a 45× coverage of real human genome
                  sequence data compresses losslessly to under 0.5
                  bits per base, allowing the 135.3 Gb of sequence to
                  fit into only 8.2 GB of space (trimming a small
                  proportion of low-quality bases from the reads
                  improves the compression still further).This is
                  \\&gt;4 times smaller than the size achieved by a
                  standard BWT-based compressor (bzip2) on the
                  untrimmed reads, but an important further advantage
                  of our approach is that it facilitates the building
                  of compressed full text indexes such as the FM-index
                  on large-scale DNA sequence
                  collections.Availability: Code to construct the BWT
                  and SAP-array on large genomic datasets is part of
                  the BEETL library, available as a github repository
                  at https://github.com/BEETL/BEETL},
  issn =	 {1367-4803},
  doi =		 {10.1093/bioinformatics/bts173},
  url =		 {https://doi.org/10.1093/bioinformatics/bts173},
  eprint =
                  {https://academic.oup.com/bioinformatics/article-pdf/28/11/1415/48868379/bioinformatics\_28\_11\_1415.pdf}
}

@Article{deorowicz11:dsrc,
  author =	 {Deorowicz, Sebastian and Grabowski, Szymon},
  title =	 "{Compression of DNA sequence reads in FASTQ format}",
  journal =	 {Bioinformatics},
  volume =	 27,
  number =	 6,
  pages =	 {860-862},
  year =	 2011,
  month =	 01,
  abstract =	 "{Motivation: Modern sequencing instruments are able
                  to generate at least hundreds of millions short
                  reads of genomic data. Those huge volumes of data
                  require effective means to store them, provide quick
                  access to any record and enable fast
                  decompression.Results: We present a specialized
                  compression algorithm for genomic data in FASTQ
                  format which dominates its competitor, G-SQZ, as is
                  shown on a number of datasets from the 1000 Genomes
                  Project (www.1000genomes.org).Availability: DSRC is
                  freely available at
                  http:/sun.aei.polsl.pl/dsrc.Contact:
                   sebastian.deorowicz@polsl.plSupplementary
                  information:  Supplementary data are available at
                  Bioinformatics online.}",
  issn =	 {1367-4803},
  doi =		 {10.1093/bioinformatics/btr014},
  url =		 {https://doi.org/10.1093/bioinformatics/btr014},
  eprint =
                  {https://academic.oup.com/bioinformatics/article-pdf/27/6/860/48863301/bioinformatics\_27\_6\_860.pdf}
}

@Article{gilmary21:compression,
  title =	 {Compression Techniques for DNA Sequences: A Thematic
                  Review.},
  author =	 {Gilmary, Rosario and Venkatesan, Akila and
                  Vaiyapuri, Govindasamy},
  journal =	 {Journal of Compututer Science and Engineering},
  volume =	 15,
  number =	 2,
  pages =	 {59--71},
  month =	 6,
  year =	 2021,
  doi =		 {0.5626/JCSE.2021.15.2.59},
  abstract =	 {Deoxyribonucleic acid (DNA) is the basic entity that
                  carries genetic instructions. This information is
                  used in the evolu- tion, progression, and
                  improvement of all species. It is estimated that 10
                  CD-ROMs are required to store the genomic data of an
                  individual being. With the increase in DNA
                  sequencing equipment, an extensive heap of genomic
                  data is cre- ated. The increase in DNA data in
                  public databases is surpassing the rate of growth in
                  storage space, thereby raising a significant concern
                  related to data storage, transmission, retrieval,
                  and search. To reduce the data storage and storage
                  expense, lossless compression procedures were
                  applied. Conventional compression methods are not
                  proficient while compressing the biological
                  data. Hence, several unique and contemporary
                  lossless compression mechanisms were used to achieve
                  improved compression ratio in biological
                  sequences. Here, we scrutinize the diverse existing
                  compression procedures that are appropriate for the
                  compression of DNA sequences. The efficiency of
                  algorithms is compared in terms of compression
                  ratio, the ratio of the capacity of the compressed
                  folder, and compression/decompression time.  Main
                  challenges and future research directions in DNA
                  compression are also presented. Emphasis has been
                  given to spe- cial references related to
                  contemporary techniques.}
}

@Article{grumbach94:new,
  title =	 {A new challenge for compression algorithms: genetic
                  sequences},
  author =	 {Grumbach, St{\'e}phane and Tahi, Fariza},
  journal =	 {Information processing \& management},
  volume =	 30,
  number =	 6,
  pages =	 {875--886},
  year =	 1994,
  url =
                  {https://inria.hal.science/inria-00180949/file/grumbach.pdf},
  publisher =	 {Elsevier},
  abstract =	 {Universal data compression algorithms fail to
                  compress genetic sequences. It is due to the
                  specificity of this particular kind of "text." We
                  analyze in some detail the properties of the
                  sequences, which cause the failure of classical
                  algorithms. We then present a lossless algorithm,
                  biocompress-2, to compress the information contained
                  in DNA and RNA sequences, based on the detection of
                  regularities, such as the presence of
                  palindromes. The algorithm combines substitutional
                  and statistical methods, and to the best of our
                  knowledge, leads to the highest compression of
                  DNA. The results, although not satisfactory, give
                  insight to the necessary correlation between
                  compression and comprehension of genetic sequences.}
}

@Article{guerra16:performance,
  title =	 {Performance comparison of sequential and parallel
                  compression applications for DNA raw data},
  author =	 {Guerra, Anibal and Lotero, Jaime and Isaza,
                  Sebastian},
  journal =	 {The Journal of Supercomputing},
  volume =	 72,
  number =	 12,
  pages =	 {4696--4717},
  year =	 2016,
  doi =		 {10.1007/s11227-016-1753-4},
  publisher =	 {Springer},
  abstract =	 {We present an experimental performance comparison of
                  lossless compression programs for DNA raw data in
                  FASTQ format files. General-purpose (PBZIP2, P7ZIP
                  and PIGZ) and domain-specific compressors (SCALCE,
                  QUIP, FASTQZ and DSRC) were analyzed in terms of
                  compression ratio, execution speed, parallel
                  scalability and memory consumption. Results showed
                  that domain-specific tools increased the compression
                  ratios up to 70\%, while reducing the runtime of
                  general-purpose tools up to 7× during compression
                  and up to 3× during decompression. Parallelism
                  scaled performance up to 13× when using 20
                  threads. Our analysis indicates that QUIP, DSRC and
                  PBZIP2 are the best tools in their respective
                  categories, with acceptable memory
                  requirements. Nevertheless, the end user must
                  consider the features of available hardware and
                  define the priorities among its optimization
                  objectives (compression ratio, runtime during
                  compression or decompression, scalability, etc.) to
                  properly select the best application for each
                  particular scenario.}
}

@PhdThesis{guerra19:efficient,
  title =	 {Efficient Storage of Genomic Sequences in High
                  Performance Computing Systems},
  author =	 {Guerra Soler, Anibal Jos{\'e}},
  year =	 2019,
  school =	 {Sistemas Embebidos e Inteligencia Computacional
                  (SISTEMIC)},
  url =
                  {https://bibliotecadigital.udea.edu.co/dspace/bitstream/10495/12525/1/GuerraSolerAnibal_2019_EfficientStorageGenomic.pdf},
  abstract =	 {In this dissertation, we address the challenges of
                  genomic data storage in high performance com- puting
                  systems. In particular, we focus on developing a
                  referential compression approach for Next Generation
                  Sequence data stored in FASTQ format files. The
                  amount of genomic data available for researchers to
                  process has increased exponentially, bringing
                  enormous challenges for its effi- cient storage and
                  transmission. General-purpose compressors can only
                  offer limited performance for genomic data, thus the
                  need for specialized compression solutions. Two
                  trends have emerged as alternatives to harness the
                  particular properties of genomic data:
                  non-referential and referential
                  compression. Non-referential compressors offer
                  higher compression rations than general purpose
                  compressors, but still below of what a referential
                  compressor could theoretically achieve. How- ever,
                  the effectiveness of referential compression depends
                  on selecting a good reference and on having enough
                  computing resources available. This thesis presents
                  one of the first referential com- pressors for FASTQ
                  files. We first present a comprehensive analytical
                  and experimental evaluation of the most relevant
                  tools for genomic raw data compression, which led us
                  to identify the main needs and opportunities in this
                  field. As a consequence, we propose a novel
                  compression work- flow that aims at improving the
                  usability of referential compressors. Subsequently,
                  we discuss the implementation and performance
                  evaluation for the core of the proposed workflow: a
                  refer- ential compressor for reads in FASTQ format
                  that combines local read-to-reference alignments
                  with a specialized binary-encoding strategy. The
                  compression algorithm, named UdeACompress, achieved
                  very competitive compression ratios when compared to
                  the best compressors in the cur- rent state of the
                  art, while showing reasonable execution times and
                  memory use. In particular, UdeACompress outperformed
                  all competitors when compressing long reads, typical
                  of the newest sequencing technologies. Finally, we
                  study the main aspects of the data-level parallelism
                  in the In- tel AVX-512 architecture, in order to
                  develop a parallel version of the UdeACompress
                  algorithms to reduce the runtime. Through the use of
                  SIMD programming, we managed to significantly ac-
                  celerate the main bottleneck found in UdeACompress,
                  the Suffix Array Construction.}
}

@Article{guerra19:tackling,
  title =	 {Tackling the challenges of FASTQ referential
                  compression},
  author =	 {Guerra, Anibal and Lotero, Jaime and Aedo, Jos{\'e}
                  {\'E}dinson and Isaza, Sebasti{\'a}n},
  journal =	 {Bioinformatics and biology insights},
  volume =	 13,
  pages =	 1177932218821373,
  year =	 2019,
  doi =		 {10.1177/1177932218821373},
  publisher =	 {SAGE Publications Sage UK: London, England},
  abstract =	 {The exponential growth of genomic data has recently
                  motivated the development of compression algorithms
                  to tackle the storage capacity limitations in
                  bioinformatics centers. Referential compressors
                  could theoretically achieve a much higher
                  compression than their non- referential
                  counterparts; however, the latest tools have not
                  been able to harness such potential yet. To reach
                  such goal, an efficient encoding model to represent
                  the differences between the input and the reference
                  is needed. In this article, we introduce a novel
                  approach for referential compression of FASTQ
                  files. The core of our compression scheme consists
                  of a referential compressor based on the combination
                  of local alignments with binary encoding optimized
                  for long reads. Here we present the algorithms and
                  performance tests developed for our reads
                  compression algorithm, named UdeACompress. Our
                  compressor achieved the best results when
                  compressing long reads and competitive compression
                  ratios for shorter reads when compared to the best
                  programs in the state of the art. As an added value,
                  it also showed reasonable execution times and memory
                  consumption, in comparison with similar tools.}
}

@InProceedings{guo13:gpu,
  title =	 {GPU-accelerated adaptive compression framework for
                  genomics data},
  author =	 {Guo, GuiXin and Qiu, Shuang and Ye, ZhiQiang and
                  Wang, BingQiang and Fang, Lin and Lu, Mian and See,
                  Simon and Mao, Rui},
  booktitle =	 {2013 IEEE International Conference on Big Data},
  pages =	 {181--186},
  year =	 2013,
  organization = {IEEE},
  doi =		 {10.1109/BigData.2013.6691572},
  abstract =	 {Genomics data is being produced at an unprecedented
                  rate, especially in the context of clinical
                  applications and grand challenge questions. There
                  are various types of data in genomics research, most
                  of which are stored as plain text tables. A data
                  compression framework tailored to this file type is
                  introduced in this paper, featuring a combination of
                  generic compression algorithms, GPU acceleration,
                  and column-major storage. This approach is the first
                  to achieve both compression and decompression rates
                  of around 100MB/s on commodity hardware without
                  compromising compression ratio. By selecting
                  appropriate compression schemes for each column of
                  data, this framework efficiently exploits data
                  redundancy while remaining applicable to a wide
                  range of formats. The GPU-accelerated implementation
                  also properly exploits the parallelism of
                  compression algorithms. Finally, this paper presents
                  a novel first-order Markov model based
                  transformation, with evidence that it is at least as
                  effective as Burrows-Wheeler and Move-To-Front in
                  some contexts.}
}

@InProceedings{gupta10:efficient,
  title =	 {Efficient storage of massive biological sequences in
                  compact form},
  author =	 {Gupta, Ashutosh and Rishiwal, Vinay and Agarwal,
                  Suneeta},
  booktitle =	 {Contemporary Computing: Third International
                  Conference},
  address =	 {Noida, India},
  pages =	 {13--22},
  month =	 8,
  year =	 2010,
  organization = {Springer},
  abstract =	 {This paper introduces a novel algorithm for DNA
                  sequence compression that makes use of a
                  transformation and statistical properties within the
                  transformed sequence. The designed compression
                  algorithm is efficient and effective for DNA
                  sequence compression. As a statistical compression
                  method, it is able to search the pattern inside the
                  compressed text which is useful in knowledge
                  discovery. Experiments show that our algorithm is
                  shown to outperform existing compressors on typical
                  DNA sequence datasets.}
}

@Article{hayden15:genome,
  title =	 {Genome researchers raise alarm over big data},
  author =	 {Hayden, Erika Check},
  journal =	 {Nature},
  volume =	 7,
  month =	 7,
  year =	 2015,
  doi =		 {10.1038/nature.2015.17912},
  publisher =	 {Nature Publishing Group},
  address =	 {New York, NY},
  abstract =	 {Storing and processing genome data will exceed the
                  computing challenges of running YouTube and Twitter,
                  biologists warn.}
}

@Article{hosseini16:survey,
  title =	 {A survey on data compression methods for biological
                  sequences},
  author =	 {Hosseini, Morteza and Pratas, Diogo and Pinho,
                  Armando J},
  journal =	 {Information},
  volume =	 7,
  number =	 4,
  pages =	 56,
  year =	 2016,
  url =		 {https://www.mdpi.com/2078-2489/7/4/56/pdf},
  doi =		 {10.3390/info7040056},
  abstract =	 {The ever increasing growth of the production of
                  high-throughput sequencing data posesa serious
                  challenge to the storage, processing and
                  transmission of these data. As frequently stated,it
                  is a data deluge.  Compression is essential to
                  address this challenge—it reduces storage spaceand
                  processing costs, along with speeding up data
                  transmission.  In this paper, we provide
                  acomprehensive survey of existing compression
                  approaches, that are specialized for biological
                  data,including protein and DNA sequences.  Also, we
                  devote an important part of the paper to
                  theapproaches proposed for the compression of
                  different file formats, such as FASTA, as well as
                  FASTQand SAM/BAM, which contain quality scores and
                  metadata, in addition to the biological
                  sequences.Then, we present a comparison of the
                  performance of several methods, in terms of
                  compression ratio,memory usage and
                  compression/decompression time.  Finally, we present
                  some suggestions forfuture research on biological
                  data compression.},
  publisher =	 {MDPI}
}

@Article{keerthy19:LZW,
  title =	 {Genomic Sequence Data Compression using
                  Lempel-Ziv-Welch Algorithm with Indexed Multiple
                  Dictionary},
  author =	 {Keerthy A. S.,Manju Priya, S.},
  journal =	 {International Journal of Engineering and Advanced
                  Technology ({IJEAT})},
  volume =	 9,
  number =	 2,
  pages =	 {541--547},
  month =	 12,
  year =	 2019,
  doi =		 {10.35940/ijeat.B3278.129219},
  url =
                  {https://www.ijeat.org/wp-content/uploads/papers/v9i2/B3278129219.pdf},
  publisher =	 {Blue Eyes Intelligence Engineering \& Sciences
                  Publication},
  abstract =	 {With the advancement in technology and development
                  of High Throughput System (HTS), the amount of
                  genomic data generated per day per laboratory across
                  the globe is surpassing the Moore’s law. The huge
                  amount of data generated is of concern to the
                  biologists with respect to their storage as well as
                  transmission across different locations for further
                  analysis. Compression of the genomic data is the
                  wise option to overcome the problems arising from
                  the data deluge. This paper discusses various
                  algorithms that exists for compression of genomic
                  data as well as a few general purpose algorithms and
                  proposes a LZW-based compression algorithm that uses
                  indexed multiple dictionaries for compression. The
                  proposed method exhibits an average compression
                  ratio of 0.41 bits per base and an average
                  compression time of 6.45 secs for a DNA sequence of
                  an average size 105.9 KB.}
},
url={
  school={University of Wisconsin-Whitewater}
}

@Article{kryukov20:sequence,
  title =	 {Sequence Compression Benchmark (SCB) database—A
                  comprehensive evaluation of reference-free
                  compressors for FASTA-formatted sequences},
  author =	 {Kryukov, Kirill and Ueda, Mahoko Takahashi and
                  Nakagawa, So and Imanishi, Tadashi},
  journal =	 {GigaScience},
  volume =	 9,
  number =	 7,
  pages =	 {giaa072},
  month =	 7,
  year =	 2020,
  doi =		 {10.1093/gigascience/giaa072},
  publisher =	 {Oxford University Press},
  abstract =	 {Background: Nearly all molecular sequence databases
                  currently use gzip for data compression. Ongoing
                  rapid accumulation of stored data calls for a more
                  efficient compression tool. Although numerous
                  compressors exist, both specialized and
                  general-purpose, choosing one of them was difficult
                  because no comprehensive analysis of their
                  comparative advantages for sequence compression was
                  available.  Findings: We systematically benchmarked
                  430 settings of 48 compressors (including 29
                  specialized sequence compressors and 19
                  general-purpose compressors) on representative
                  FASTA-formatted datasets of DNA, RNA, and protein
                  sequences. Each compressor was evaluated on 17
                  performance measures, including compression
                  strength, as well as time and memory required for
                  compression and decompression. We used 27 test
                  datasets including individual genomes of various
                  sizes, DNA and RNA datasets, and standard protein
                  datasets. We summarized the results as the Sequence
                  Compression Benchmark database (SCB database,
                  http://kirr.dyndns.org/sequence-compression-benchmark/),
                  which allows custom visualizations to be built for
                  selected subsets of benchmark results.  Conclusion:
                  We found that modern compressors offer a large
                  improvement in compactness and speed compared to
                  gzip. Our benchmark allows compressors and their
                  settings to be compared using a variety of
                  performance measures, offering the opportunity to
                  select the optimal compressor on the basis of the
                  data type and usage scenario specific to a
                  particular application.}
}

@InProceedings{lahaise02:aio,
  title =	 {An AIO Implementation and its Behaviour},
  author =	 {LaHaise, Benjamin CR},
  booktitle =	 {Ottawa Linux Symposium},
  pages =	 260,
  year =	 2002
}

@InProceedings{lin09:compressed,
  title =	 {Compressed pattern matching in dna sequences using
                  multithreaded technology},
  author =	 {Lin, Piyuan and Liu, Shaopeng and Zhang, Lixia and
                  Huang, Peijie},
  booktitle =	 {Proceedings of the 3rd International Conference on
                  Bioinformatics and Biomedical Engineering},
  pages =	 {1--4},
  month =	 6,
  year =	 2009,
  doi =		 {10.1109/ICBBE.2009.5162550},
  organization = {IEEE},
  abstract =	 {Compressed pattern matching on large DNA sequences
                  data is very important in bioinformatics. In this
                  paper, in order to improve the performance by
                  searching pattern in parallel time, multithreaded
                  programming technique is used. Then, two novel
                  multithreaded algorithms are proposed, named MTd-BM
                  and MTd-Horspool. The first one is a mutation of
                  d-BM algorithm, which is based on Boyer-Moore
                  method. And the second one is designed in the
                  similitude of MTd-BM, but using Horspool method as
                  its foundation. The experimental results show that
                  these two algorithms are nearly 2 times faster than
                  the d-BM algorithm for long DNA pattern
                  (length>50). Moreover, compression of DNA sequences
                  gives a guaranteed space saving of 75\%.}
}

@InProceedings{majumder18:cbstd,
  title =	 {{CBSTD}: a cloud based symbol table driven dna
                  compression algorithm},
  author =	 {Majumder, Annwesha Banerjee and Gupta, Somsubhra},
  booktitle =	 {Industry Interactive Innovations in Science,
                  Engineering and Technology: Proceedings of the
                  International Conference, I3SET 2016},
  pages =	 {467--476},
  year =	 2018,
  organization = {Springer},
  doi =		 {10.1007/978-981-10-3953-9_45},
  abstract =	 {n this paper, we propose symbol table driven DNA
                  compression algorithm aimed to use as a cloud
                  service. Bioinformatics requires a huge amount of
                  genomic data to analysis, so optimal storage and
                  compression is a great challenge to this field. We
                  categorized the DNA sequence into three different
                  parts according to the occurrence of A T C and G and
                  use two different symbols tables to map the DNA
                  sequences into a compressed sequence. We are
                  intended to deploy our proposed compression
                  algorithm in cloud, so that the user of this field
                  can access this Software as a Service over the
                  cloud. Through our proposed method of compression,
                  we claim to achieve a compression rate of 1.82.}
}

@Article{mansouri20:lossless,
  title =	 {A new lossless DNA compression algorithm based on a
                  single-block encoding scheme},
  author =	 {Mansouri, Deloula and Yuan, Xiaohui and Saidani,
                  Abdeldjalil},
  journal =	 {Algorithms},
  volume =	 13,
  number =	 4,
  pages =	 99,
  year =	 2020,
  doi =		 {10.3390/a13040099},
  publisher =	 {MDPI},
  abstract =	 {With the emergent evolution in DNA sequencing
                  technology, a massive amount of genomic data is
                  produced every day, mainly DNA sequences, craving
                  for more storage and bandwidth. Unfortunately,
                  managing, analyzing and specifically storing these
                  large amounts of data become a major scientific
                  challenge for bioinformatics. Therefore, to overcome
                  these challenges, compression has become
                  necessary. In this paper, we describe a new
                  reference-free DNA compressor abbreviated as
                  DNAC-SBE. DNAC-SBE is a lossless hybrid compressor
                  that consists of three phases. First, starting from
                  the largest base (Bi), the positions of each Bi are
                  replaced with ones and the positions of other bases
                  that have smaller frequencies than Bi are replaced
                  with zeros. Second, to encode the generated streams,
                  we propose a new single-block encoding scheme (SEB)
                  based on the exploitation of the position of
                  neighboring bits within the block using two
                  different techniques. Finally, the proposed
                  algorithm dynamically assigns the shorter length
                  code to each block. Results show that DNAC-SBE
                  outperforms state-of-the-art compressors and proves
                  its efficiency in terms of special conditions
                  imposed on compressed data, storage space and data
                  transfer rate regardless of the file format or the
                  size of the data.}
}

@Article{mehta10:hash,
  title =	 {Dna compression using hash based data structure},
  author =	 {Mehta, Ateet and Patel, Bankim},
  journal =	 {International Journal of Information Technology \&
                  Knowledge Management},
  volume =	 3,
  pages =	 {383--386},
  year =	 2010,
  publisher =	 {CS Journals},
  address =	 {Haryana, India},
  abstract =	 {DNA Sequences making up any organism comprise the
                  basic blueprint of that organism so that
                  understanding and analyzing different genes within
                  sequences has become an extremely important
                  task. Biologists are producing huge volumes of DNA
                  sequences every day that makes genome sequence
                  database growing exponentially. The databases such
                  as EMBL, GenBank represent millions of DNA sequences
                  filling many thousands of gigabytes computer storage
                  capacity and the databases are doubled in size every
                  6-8 months. Hence an efficient algorithm to compress
                  DNA sequence is required. Though there are many text
                  compression algorithms, they are not well suited for
                  the characteristics of DNA sequences. There are
                  algorithms for DNA compression which takes advantage
                  of repetitive nature of DNA fragments within the
                  sequence where as few of the other algorithms are
                  written for the non repeated patterns within DNA
                  sequences. In this paper, we represent an algorithm
                  which is based on hash based data structure to
                  compress DNA sequences. The proposed algorithm
                  performs equally well for both repeated and
                  non-repeated patterns within the DNA sequence.},
}

@Article{mitra18:survey,
  title =	 {A Survey of Genome Compression Methodology},
  author =	 {Mitra, Rituparna and Roy, Subhankar},
  journal =	 {International Journal of Computer Science and
                  Engineering},
  volume =	 6,
  pages =	 {983--991},
  month =	 8,
  year =	 2018,
  doi =		 {10.26438/ijcse/v6i8.983991},
  publisher =	 {SSRG},
  address =	 {Indore, India}
}

@Article{nishad12:vital,
  title =	 {A vital approach to compress the size of DNA
                  sequence using LZW (Lempel-Ziv-Welch) with fixed
                  length binary code and tree structure},
  author =	 {Nishad, PM and Chezian, R Manicka},
  journal =	 {International Journal of Computer Applications},
  volume =	 43,
  number =	 1,
  pages =	 {7--9},
  year =	 2012,
  publisher =	 {CS Journals},
  abstract =	 {The genome of an organism contains all hereditary
                  information encoded in Deoxyribonucleic Acid (DNA).
                  Molecular sequence databases (e.g.,EMBL, Genbank,
                  DDJB, Entrez, SwissProt, etc) represent millions of
                  DNA sequences filling many thousands of gigabytes
                  and the databases are doubled in size every 6-8
                  months, which may go to beyond the limit of storage
                  capacity. There are several text compression
                  algorithm used for DNA compression. This paper
                  proposes a new hybrid algorithm is used to compress
                  DNA sequence, the algorithm is designed by combining
                  the fixed length binary code with the LZW
                  (Lempel-Ziv-Welch) compression algorithm. Initially
                  the input sequence is divided in to fragments where
                  each fragment consist of four nucleotides and fixed
                  length binary code is assigned to each nucleotide
                  then the pattern (STR and CHR) in LZW used the same
                  for creating the dictionary. Assigning a new binary
                  code for each pattern in the dictionary using a
                  binary tree, and the sequence is replaced binary
                  code for the longest match in the dictionary while
                  compression. The proposed approach attains maximum
                  compression in DNA sequences.}
}

@Article{nishad13:optimization,
  title =	 {Optimization of {LZW} (Lempel-Ziv-Welch) Algorithm
                  to Reduce Time Complexity for Dictionary Creation in
                  Encoding and Decoding},
  author =	 {Nishad, PM and Chezian, R Manicka},
  journal =	 {Asian Journal of Computer Science and Information
                  Technology},
  volume =	 5,
  pages =	 {114--118},
  month =	 10,
  year =	 2013,
  abstract =	 {LZW (Lempel-Ziv-Welch) is a universal lossless data
                  compression algorithm, which takes linear time in
                  encoding and decoding. This paper discusses a
                  methodology to reduce time complexity by combining
                  binary search with LZW. The existing LZW compression
                  algorithm takes large time for dictionary creation
                  while encoding and decoding. By using binary search
                  with LZW the time complexity can be reduced
                  optimally and gradually because the comparison ratio
                  is less while creating the dictionary. Especially
                  while compressing the search for patterns in the
                  table and also in the decompression algorithm for
                  finding the pattern in the table is taking linear
                  time for searching. Therefore, combining Enhanced
                  LZW with binary search reduces the time
                  complexity. The proposed methodology may be bust in
                  data compression for communication, Maximizes the
                  reduction of complexity in pattern identification
                  for compression. The proposed methodology reduces
                  the complexity in time with Binary search tree
                  (BST). The experimental result shows 94.21\%
                  improvement on Compression and 93.34\% improvement
                  on Decompression.}
}

@InProceedings{ouyang12:fast,
  title =	 {Fast compression of huge DNA sequence data},
  author =	 {Ouyang, Jichao and Feng, Ping and Kang, Jichang},
  booktitle =	 {Proceedings of the 5th International Conference on
                  BioMedical Engineering and Informatics},
  pages =	 {885--888},
  year =	 2012,
  organization = {IEEE},
  doi =		 {10.1109/BMEI.2012.6512909},
  abstract =	 {DNA sequences can be enormous in size. There have
                  been several DNA sequence oriented compression
                  methods like Biocompress, DNACompress, Cfact,
                  CTW+LZ, and DNADP. These compression methods can
                  achieve high compression ratio, but sacrifice too
                  much of time. For example, CTW+LZ takes several
                  hours to compress a sequence HEMCMVCG of 227
                  KB. DNADP takes about 20 minutes to compress
                  standard benchmark sequences. Here we introduce an
                  improved RLE method, which has lower computation
                  complex. Thus, it significantly improves the running
                  time against previous DNA compression programs. Our
                  improved LRE can achieve compression ratio of 1.862
                  bits per base. It only takes about 1 minute on a 2.1
                  GHz Core 2 duo processor to compress a 250MB
                  chromosomes sequence file. And we use the Delta
                  Encoding to reduce the second sequence to 4.8MB.}
}

@InProceedings{pingali17:gpu,
  title =	 {GPU accelerated suffix array construction for large
                  genome sequences},
  author =	 {Pingali, Kartheek Diwakar and Tanay, K Chaitanya
                  Pavan and Baruah, Pallav Kumar},
  booktitle =	 {International Conference on Innovations in
                  Information, Embedded and Communication Systems
                  ({ICIIECS})},
  pages =	 {1--6},
  month =	 3,
  year =	 2017,
  doi =		 {10.1109/ICIIECS.2017.8276035},
  organization = {IEEE},
  abstract =	 {Next generation sequencing technologies have made it
                  possible to sequence the genome of an organism in
                  hours. Genome is basically a string made of four
                  amino acids represented as A, C, G and T. Only these
                  four amino acids make up for the complete DNA
                  sequence of an organism. These genome sequences are
                  huge in size and requires efficient techniques to
                  analyze them. Well known data structures used to
                  analyze genomes include suffix array/tree, fm-index,
                  burrows wheeler transform etc. In computational
                  biology, pattern matching is often used to
                  efficiently detect the presence of a gene sequence
                  within the genome sequence. This type of sequence
                  analysis requires very fast methods to construct
                  suffix array for the genome sequences. In this paper
                  we have proposed a GPU based method to construct
                  suffix arrays efficiently. We have constructed
                  suffix arrays for homo sapiens chromosomes of
                  GRCh38.p7 primary assembly on varied sizes of upto
                  1.4GB and we have achieved a significant speed up of
                  upto 3x against the best sequential implementation.}
}

@InProceedings{pratas19:dna,
  title =	 {A DNA sequence corpus for compression benchmark},
  author =	 {Pratas, Diogo and Pinho, Armando J},
  booktitle =	 {Practical Applications of Computational Biology and
                  Bioinformatics, 12th International Conference},
  pages =	 {208--215},
  year =	 2019,
  organization = {Springer}
}

@Article{pratas19:reference,
  title =	 {A reference-free lossless compression algorithm for
                  DNA sequences using a competitive prediction of two
                  classes of weighted models},
  author =	 {Pratas, Diogo and Hosseini, Morteza and Silva, Jorge
                  M and Pinho, Armando J},
  journal =	 {Entropy},
  volume =	 21,
  number =	 11,
  pages =	 1074,
  month =	 11,
  year =	 2019,
  doi =		 {10.3390/e21111074},
  publisher =	 {MDPI},
  abstract =	 {The development of efficient data compressors for
                  DNA sequences is crucial not only for reducing the
                  storage and the bandwidth for transmission, but also
                  for analysis purposes. In particular, the
                  development of improved compression models directly
                  influences the outcome of anthropological and
                  biomedical compression-based methods. In this paper,
                  we describe a new lossless compressor with improved
                  compression capabilities for DNA sequences
                  representing different domains and kingdoms. The
                  reference-free method uses a competitive prediction
                  model to estimate, for each symbol, the best class
                  of models to be used before applying arithmetic
                  encoding. There are two classes of models: weighted
                  context models (including substitutional tolerant
                  context models) and weighted stochastic repeat
                  models. Both classes of models use specific
                  sub-programs to handle inverted repeats
                  efficiently. The results show that the proposed
                  method attains a higher compression ratio than
                  state-of-the-art approaches, on a balanced and
                  diverse benchmark, using a competitive level of
                  computational resources. An efficient implementation
                  of the method is publicly available, under the GPLv3
                  license.}
}

@InProceedings{priyanka14:ascii,
  title =	 {A compression algorithm for DNA that uses ASCII
                  values},
  author =	 {Goel, Savita and others},
  booktitle =	 {{IEEE} International Advance Computing Conference
                  ({IACC})},
  pages =	 {739--743},
  year =	 2014,
  organization = {IEEE},
  doi =		 {10.1109/IAdCC.2014.6779416},
  abstract =	 {The properties of DNA sequences offer an opportunity
                  to develop DNA specific compression algorithm. A
                  lossless two phase compression algorithm is
                  presented for DNA sequences. In the first phase a
                  modified version of Run Length Encoding (RLE) is
                  applied and in the second phase the resultant
                  genetic sequences is compressed using ASCII
                  values. Using ASCII codes for eight bits ensures
                  one-fourth compression irrespective of repeated or
                  non-repeated behavior of the sequence and modified
                  RLE technique enhances the compression further
                  more. Not only the compression ratio of the
                  algorithm is quite encouraging but the simple
                  technique of compression makes it more interesting.}
}

@Article{reuter15:high,
  title =	 {High-throughput sequencing technologies},
  author =	 {Reuter, Jason A and Spacek, Damek V and Snyder,
                  Michael P},
  journal =	 {Molecular cell},
  volume =	 58,
  number =	 4,
  pages =	 {586--597},
  month =	 5,
  year =	 2015,
  doi =		 {10.1016/j.molcel.2015.05.004},
  publisher =	 {Elsevier},
  abstract =	 {The human genome sequence has profoundly altered our
                  understanding of biology, human diversity, and
                  disease. The path from the first draft sequence to
                  our nascent era of personal genomes and genomic
                  medicine has been made possible only because of the
                  extraordinary advancements in DNA sequencing
                  technologies over the past 10 years. Here, we
                  discuss commonly used high-throughput sequencing
                  platforms, the growing array of sequencing assays
                  developed around them, as well as the challenges
                  facing current sequencing platforms and their
                  clinical application.}
}

@InProceedings{saada15:dna,
  title =	 {DNA sequences compression algorithm based on
                  extended-ASCII representation},
  author =	 {Saada, Bacem and Zhang, Jing},
  booktitle =	 {Proceedings of the world congress on engineering and
                  computer science},
  month =	 2,
  address =	 {San Francisco, CA},
  url =
                  {www.iaeng.org/publication/WCECS2015/WCECS2015_pp556-560.pdf},
  year =	 2015,
  abstract =	 {The large amount of DNA sequences stored in
                  databases has led researchers to propose compression
                  algorithms for DNA sequences. The properties of the
                  DNA sequences offer the opportunity to use a
                  LOSSLESS algorithm. In this paper, we will present a
                  two phases algorithm based on the binary
                  representation of DNA sequences. In the first phase,
                  we will compress the DNA sequences using the
                  Extended-ASCII encoding through which one character
                  encode four nucleotides.  Thereafter, we will apply
                  the Run Length Encoding technique to further enhance
                  the compression of entire genomes. The simple way to
                  implement the algorithm and its remarkable
                  compression ratio make it interesting to be used.}
}

@InProceedings{satyanvesh12:gencodex,
  title =	 {GenCodex-A novel algorithm for compressing DNA
                  sequences on multi-cores and GPUs},
  author =	 {Satyanvesh, D and Balleda, Kaliuday and Padyana,
                  Ajith and Baruah, PK},
  booktitle =	 {Proceedings of the 19th International Conference on
                  High Performance Computing (HiPC)},
  address =	 {Pune, India},
  organization = {IEEE},
  year =	 2012,
  abstract =	 {The DNA sequences are huge in size and the databases
                  are growing at an exponential rate. For example, the
                  human genome in raw format ranges from 2 to 30
                  Tera-bytes. The main reason for this is the
                  invention of new species and increasing number of
                  DNA profiles. The growth of the DNA affects the
                  storage as well as bandwidth when these sequences
                  need to be transferred. Applications such as DNA
                  profiling, Real time DNA crime investiga- tion
                  require access to the DNA sequences in real time.
                  The inherent property of DNA is that it contains
                  many repeats which makes it highly
                  compressible. However, the applications mentioned
                  not only require good compression ratio but also
                  needs faster compression. Multi-cores and GPUs can
                  be used to perform the compression quickly.  In this
                  paper, we propose a new algorithm with a focus on
                  the throughput along with the compression ratio. The
                  algorithm scales well on GPUs and achieves a speedup
                  of 11 on multi-cores and upto 23 on GPUs.}
}

@PhdThesis{schoeller21:simple,
  title =	 {A simple, space-efficient algorithm for detecting
                  start and stop codons in DNA},
  author =	 {Schoeller, Scott J},
  year =	 2021,
  url =
                  {https://minds.wisconsin.edu/bitstream/handle/1793/82249/ScottSchoellerThesisV4.pdf},
}

@Article{schuster08:next,
  title =	 {Next-generation sequencing transforms today's
                  biology},
  author =	 {Schuster, Stephan C},
  journal =	 {Nature methods},
  volume =	 5,
  number =	 1,
  pages =	 {16--18},
  month =	 12,
  year =	 2008,
  doi =		 {10.1038/nmeth1156},
  publisher =	 {Nature Publishing Group},
  address =	 {New York, NY},
  abstract =	 {A new generation of non-Sanger-based sequencing
                  technologies has delivered on its promise of
                  sequencing DNA at unprecedented speed, thereby
                  enabling impressive scientific achievements and
                  novel biological applications. However, before
                  stepping into the limelight, next-generation
                  sequencing had to overcome the inertia of a field
                  that relied on Sanger-sequencing for 30 years.}
}

@Article{stephens15:big,
  title =	 {Big data: astronomical or genomical?},
  author =	 {Stephens, Zachary D and Lee, Skylar Y and Faghri,
                  Faraz and Campbell, Roy H and Zhai, Chengxiang and
                  Efron, Miles J and Iyer, Ravishankar and Schatz,
                  Michael C and Sinha, Saurabh and Robinson, Gene E},
  journal =	 {PLoS biology},
  volume =	 13,
  number =	 7,
  pages =	 {e1002195},
  month =	 7,
  year =	 2015,
  doi =		 {10.1371/journal.pbio.1002195},
  publisher =	 {Public Library of Science},
  address =	 {San Francisco, CA},
  abstract =	 {Genomics is a Big Data science and is going to get
                  much bigger, very soon, but it is not known whether
                  the needs of genomics will exceed other Big Data
                  domains. Projecting to the year 2025, we compared
                  genomics with three other major generators of Big
                  Data: astronomy, YouTube, and Twitter. Our estimates
                  show that genomics is a “four-headed beast”—it is
                  either on par with or the most demanding of the
                  domains analyzed here in terms of data acquisition,
                  storage, distribution, and analysis. We discuss
                  aspects of new technologies that will need to be
                  developed to rise up and meet the computational
                  challenges that genomics poses for the near
                  future. Now is the time for concerted,
                  community-wide planning for the “genomical”
                  challenges of the next decade.}
}

@Misc{ucsc:2bit,
  howpublished = {https://genome.ucsc.edu/goldenPath/help/twoBit.html},
  note =	 {visited on 2023-07-16},
}

@Article{wandelt14:trends,
  title =	 {Trends in genome compression},
  author =	 {Wandelt, Sebastian and Bux, Marc and Leser, Ulf},
  journal =	 {Current Bioinformatics},
  volume =	 9,
  number =	 3,
  pages =	 {315--326},
  year =	 2014,
  publisher =	 {Bentham Science Publishers},
  address =	 {Sharjah, UAE},
  abstract =	 {Technological advancements in high throughput
                  sequencing have led to a tremendous increase in the
                  amount of genomic data produced. With the cost being
                  down to 2,000 USD for a single human genome,
                  sequencing dozens of individuals is an undertaking
                  that is feasible even for a smaller projects or
                  organizations established. However, generating the
                  sequence is only one issue; another one is storing,
                  managing, and analyzing it. These tasks become more
                  and more challenging due to the sheer size of the
                  data sets and are increasingly considered to be the
                  major bottlenecks in larger genome projects. One
                  possible countermeasure is to compress the data;
                  compression reduces costs in terms of requiring less
                  hard disk storage and in terms of requiring less
                  bandwidth if data is shipped to large compute
                  clusters for parallel analysis. Accordingly,
                  sequence compression has recently attracted much
                  interest in the scientific community. In this paper,
                  we explain the different basic techniques for
                  sequence compression, point to distinctions between
                  different compression tasks (e.g., genome
                  compression versus read compression), and present a
                  comparison of current approaches and tools. To
                  further stimulate progress in genome compression
                  research, we also identify key challenges for future
                  systems.}
}

@Article{zhu13:throughput,
  author =	 {Zhu, Zexuan and Zhang, Yongpeng and Ji, Zhen and He,
                  Shan and Yang, Xiao},
  title =	 "{High-throughput DNA sequence data compression}",
  journal =	 {Briefings in Bioinformatics},
  volume =	 16,
  number =	 1,
  pages =	 {1-15},
  year =	 2013,
  month =	 12,
  abstract =	 "{The exponential growth of high-throughput DNA
                  sequence data has posed great challenges to genomic
                  data storage, retrieval and
                  transmission. Compression is a critical tool to
                  address these challenges, where many methods have
                  been developed to reduce the storage size of the
                  genomes and sequencing data (reads, quality scores
                  and metadata). However, genomic data are being
                  generated faster than they could be meaningfully
                  analyzed, leaving a large scope for developing novel
                  compression algorithms that could directly
                  facilitate data analysis beyond data transfer and
                  storage. In this article, we categorize and provide
                  a comprehensive review of the existing compression
                  methods specialized for genomic data and present
                  experimental results on compression ratio, memory
                  usage, time for compression and decompression. We
                  further present the remaining challenges and
                  potential directions for future research.}",
  issn =	 {1467-5463},
  doi =		 {10.1093/bib/bbt087},
  url =		 {https://doi.org/10.1093/bib/bbt087},
  eprint =
                  {https://academic.oup.com/bib/article-pdf/16/1/1/5040812/bbt087.pdf}
}

@InProceedings{wang18:deepdna,
  title={DeepDNA: A hybrid convolutional and recurrent neural network for compressing human mitochondrial genomes},
  author={Wang, Rongjie and Bai, Yang and Chu, Yan-Shuo and Wang, Zhenxing and Wang, Yongtian and Sun, Mingrui and Li, Junyi and Zang, Tianyi and Wang, Yadong},
  booktitle={International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages={270--274},
  month=dec,
  year=2018,
  doi={10.1109/BIBM.2018.8621140},
  organization={IEEE},
  abstract={Large amounts of genome data are publicly available due to the high-throughput sequencing technologies developed in recent years. This availability raises a major concern about data storage costs, given that an effective and efficient compression algorithm for genome data remains an unresolved challenge in genomic data studies. In this paper, we propose a compression method, DeepDNA, that is a hybrid convolutional and recurrent deep neural network for compressing human genome data. In the DeepDNA model, the convolutional layer captures the genome's local features, while the recurrent layer captures long-term dependencies for estimating the next base probabilities in the genomic sequence. The experimental results on human mitochondrial genome datasets show the effectiveness of the DeepDNA method.The code for DeepDNA is available at https://github.com/rongiiewang/deepDNA.}
}

@Article{du20:compression,
  title={A compression method for DNA},
  author={Du, Shengwang and Li, Junyi and Bian, Naizheng},
  journal={Plos one},
  volume={15},
  number={11},
  pages={e0238220},
  month=nov,
  year={2020},
  doi={10.1371/journal.pone.0238220},
  publisher={Public Library of Science San Francisco, CA USA}
}
